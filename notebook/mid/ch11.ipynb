{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00366e5b-f19a-4c4e-9422-7f56022d5eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "california = fetch_california_housing()\n",
    "\n",
    "df = pd.DataFrame(california.data, columns = california.feature_names)\n",
    "df[\"Target\"] = california.target\n",
    "df.tail()\n",
    "\n",
    "# 데이터 정규화\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df.values[:,:-1])\n",
    "df.iloc[:,:-1] = scaler.transform(df.values[:,:-1]).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c38d7fd-ec61-4e83-a319-bf41fac49be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20640, 8]) torch.Size([20640, 1])\n"
     ]
    }
   ],
   "source": [
    "# 학습코드 구현\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "data = torch.from_numpy(df.values).float()\n",
    "\n",
    "x = data[:,:-1]\n",
    "y = data[:,-1:]\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20c61ed3-2f40-4879-8b55-b18357faecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "n_epochs = 4000\n",
    "batch_size = 256 # 미니배치 크기(한번에 처리하는 데이터 샘플 개수)\n",
    "print_interval = 200\n",
    "# learning_rate = 1e-2 ---> 학습률 필요 없음(초기에 큰 학습률을 가지면서 점차 감쇠 + 매번 계산된 그래디언트를 누적하는 형태구현 : 아담)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89d4d69a-eeb1-4756-b4fa-08568499eeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 심층신경망 정의 (너비 : 6, 깊이 : 5)\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(x.size(-1),6),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(6,5),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(5,4),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(4,3),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(3,y.size(-1)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c02c3ee1-2bea-450d-9646-f02b2089922f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters()) # 인자에 학습률은 넣지 않음 (각 파라미터마다 학습률을 자동으로 조정하기 때문, 기본 학습률 : 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15493c1f-6c27-46e1-bbe2-14e7c1334a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dc/3dqm8vys2qv2c52_q3sn76xr0000gn/T/ipykernel_11756/2139505422.py:23: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:836.)\n",
      "  total_loss += float(loss) # 메모리 손실을 없애기 위해\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200: loss=3.1970e-01\n",
      "Epoch 400: loss=3.1041e-01\n",
      "Epoch 600: loss=3.0580e-01\n",
      "Epoch 800: loss=2.9977e-01\n",
      "Epoch 1000: loss=2.9722e-01\n",
      "Epoch 1200: loss=2.9475e-01\n",
      "Epoch 1400: loss=2.9307e-01\n",
      "Epoch 1600: loss=2.9338e-01\n",
      "Epoch 1800: loss=2.9256e-01\n",
      "Epoch 2000: loss=2.9235e-01\n",
      "Epoch 2200: loss=2.9245e-01\n",
      "Epoch 2400: loss=2.9170e-01\n",
      "Epoch 2600: loss=2.9293e-01\n",
      "Epoch 2800: loss=2.9206e-01\n",
      "Epoch 3000: loss=2.9205e-01\n",
      "Epoch 3200: loss=2.9118e-01\n",
      "Epoch 3400: loss=2.9199e-01\n",
      "Epoch 3600: loss=2.9288e-01\n",
      "Epoch 3800: loss=2.9096e-01\n",
      "Epoch 4000: loss=2.9124e-01\n",
      "Elapsed time: 1m 66s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "for i in range(n_epochs):\n",
    "    indices = torch.randperm(x.size(0))\n",
    "    x_ = torch.index_select(x, dim=0, index=indices)\n",
    "    y_ = torch.index_select(y, dim=0, index=indices)\n",
    "\n",
    "    x_ = x_.split(batch_size, dim=0)\n",
    "    y_ = y_.split(batch_size, dim=0)\n",
    "\n",
    "    y_hat = []\n",
    "    total_loss = 0\n",
    "\n",
    "    for x_i, y_i in zip(x_,y_):\n",
    "        y_hat_i = model(x_i)\n",
    "        loss = F.mse_loss(y_hat_i, y_i)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss) # 메모리 손실을 없애기 위해\n",
    "\n",
    "        y_hat += [y_hat_i]\n",
    "\n",
    "    total_loss = total_loss / len(x_)\n",
    "    if(i+1) % print_interval == 0:\n",
    "        print('Epoch %d: loss=%.4e' % (i+1, total_loss))\n",
    "\n",
    "y_hat = torch.cat(y_hat, dim=0)\n",
    "y = torch.cat(y_, dim=0)\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print('Elapsed time: %dm %ds'%(elapsed//60, int(elapsed - elapsed//60)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e066d51b-091e-433e-90d2-d7a47fdfd22c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
